{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome To Data Mining \u00b6 Biodata \u00b6 Nama : Muhammad Syahrullah NIM : 180411100006 Kelas : Penambangan Data (5D) Jurusan : Teknik Informatika Angkatan : 2018 Alamat : Bekasi \u200b","title":"index"},{"location":"#welcome-to-data-mining","text":"","title":"Welcome To Data Mining"},{"location":"#biodata","text":"Nama : Muhammad Syahrullah NIM : 180411100006 Kelas : Penambangan Data (5D) Jurusan : Teknik Informatika Angkatan : 2018 Alamat : Bekasi \u200b","title":"Biodata"},{"location":"Decision Tree/","text":"DECISION TREE \u00b6 decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Decision Tree"},{"location":"Decision Tree/#decision-tree","text":"decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"Decision Tree/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"CARA MEMBUAT DECISION TREE"},{"location":"Isi/","text":"Statistika Deskriptif \u00b6 pengertian: Statistik Deskriptif adalah statistik yang berfungsi untuk mendeskripsikan atau memberi gambaran terhadap obyek yang diteliti melalui data sampel atau populasi sebagaimana adanya, tanpa melakukan analisis dan membuat kesimpulan yang berlaku untuk umum. Tipe Statistik Deskriptif : Mean \u00b6 \u200b Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Median \u00b6 \u200b Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Nilai tengah dari kelompok data n = banyak data Modus \u00b6 \u200b Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif Varians \u00b6 \u200b Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data Standard Deviasi \u00b6 \u200b Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Rumus Standar Deviasi: $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Dimana : xi = nilai x ke i x = rata rata n = ukuran sampel Skawness \u00b6 Skawness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan mirin atau tidaknya bentuk kurva suatu distribusi frekuensi. $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile \u00b6 Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3 $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = nilai quartil n = banyak data Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan : \u00b6 Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Langkah - Langkah \u00b6 PERTAMA \u00b6 Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd KEDUA \u00b6 Selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'data.csv' ) df setelah di run maka program akan menampilkan seperti berikut : stats MTK B.indonesia B.inggris Fisika Min 20 50 50 40 Max 100 100 100 100 Mean 59.798 74.834 74.398 69.672 Standard Deviasi 23.37 14.77 14.72 17.35 Variasi 546.34 218.02 216.64 300.97 SKewnes 0.01 0.02 0.06 -0.02 Quartile 1 40 62 61 55 Quartile 2 59 74 73 70.5 Quartile 3 80.25 88 88 84 Median 59 74 73 70.5 Modus 39 60 95 56 KETIGA \u00b6 Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) KEEMPAT \u00b6 Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistika"},{"location":"Isi/#statistika-deskriptif","text":"pengertian: Statistik Deskriptif adalah statistik yang berfungsi untuk mendeskripsikan atau memberi gambaran terhadap obyek yang diteliti melalui data sampel atau populasi sebagaimana adanya, tanpa melakukan analisis dan membuat kesimpulan yang berlaku untuk umum. Tipe Statistik Deskriptif :","title":"Statistika Deskriptif"},{"location":"Isi/#mean","text":"\u200b Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Mean"},{"location":"Isi/#median","text":"\u200b Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Nilai tengah dari kelompok data n = banyak data","title":"Median"},{"location":"Isi/#modus","text":"\u200b Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif","title":"Modus"},{"location":"Isi/#varians","text":"\u200b Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data","title":"Varians"},{"location":"Isi/#standard-deviasi","text":"\u200b Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Rumus Standar Deviasi: $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Dimana : xi = nilai x ke i x = rata rata n = ukuran sampel","title":"Standard Deviasi"},{"location":"Isi/#skawness","text":"Skawness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan mirin atau tidaknya bentuk kurva suatu distribusi frekuensi. $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skawness"},{"location":"Isi/#quartile","text":"Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3 $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = nilai quartil n = banyak data","title":"Quartile"},{"location":"Isi/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"Isi/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan :"},{"location":"Isi/#langkah-langkah","text":"","title":"Langkah - Langkah"},{"location":"Isi/#pertama","text":"Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd","title":"PERTAMA"},{"location":"Isi/#kedua","text":"Selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'data.csv' ) df setelah di run maka program akan menampilkan seperti berikut : stats MTK B.indonesia B.inggris Fisika Min 20 50 50 40 Max 100 100 100 100 Mean 59.798 74.834 74.398 69.672 Standard Deviasi 23.37 14.77 14.72 17.35 Variasi 546.34 218.02 216.64 300.97 SKewnes 0.01 0.02 0.06 -0.02 Quartile 1 40 62 61 55 Quartile 2 59 74 73 70.5 Quartile 3 80.25 88 88 84 Median 59 74 73 70.5 Modus 39 60 95 56","title":"KEDUA"},{"location":"Isi/#ketiga","text":"Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' )))","title":"KETIGA"},{"location":"Isi/#keempat","text":"Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"KEEMPAT"},{"location":"Mengukur Jarak Data/","text":"Mengukur Jarak Data \u00b6 Pengukuran jarak merupakan basis dalam pemetaan. Mengukur Jarak Tipe Numerik \u00b6 Minkowski Distance \u00b6 Minkowski Distanceyang merupakan generalisasi dari beberapa distance space yang ada seperti L1 (Manhattan/City Block) dan L2 (Euclidean), juga telah diimplementasikan. Minkowski menggunakan bilangan order \u03bb. $$ d _ { \\operatorname { min } } = ( \\sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ keterangan: (m) = bilangan riel positif (x_i) dan (y_i) = dua vektor dalam runang dimensi (n) Manhattan distance \u00b6 Manhattan Distance adalah merupakan salah satu teknik yang sering digunakan untuk menentukan kesamaan antara dua buah obyek. Pengukuran ini dihasilkan berdasarkan penjumlahan jarak selisih antara dua buah obyek dan hasil yang didapatkan dari Manhattan Distance bernilai mutlak. Dimana, Manhattan Distance melakukan perhitungan jarak dengan cara tegak lurus. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ keterangan : (x_i) = nilai X pada data latih. (y_i) = nilai Y pada data uji. m = batas jumlah banyaknya data Jika angka hasil dari rumus tersebut besar, maka tingkatkemiripan antara kedua objek akan semakin kecil dan sebaliknya, jika angka hasil rumus tersebut kecil, maka tingkat kemiripanantara kedua objek akan semakin besar. Objek yang dimaksudadalah data latih dan data uji yang akan dihitung tingkatkemiripannya. Euclidean distance \u00b6 Euclidean distance adalah perhitungan jarak dari 2 buah titik dalam Euclidean space. Euclidean space diperkenalkan oleh Euclid, seorang matematikawan dari Yunani sekitar tahun 300 B.C.E. untuk mempelajari hubungan antara sudut dan jarak. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Berikut adalah persamaan EuclideanDistanceuntuk titik yang mempunyai ruang 3 dimensi (Negoro, 2015): Average distance \u00b6 adalah modifikasi dari euclidean distance yang dibuat karena adanya kelemahan dari euclidean distance itu sendiri. $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ keterangan : (x,y) = titik koordinat dari vektor (n)= dimensi Weighted euclidean distance \u00b6 adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ keterangan: (w_i) = bobot yang diberikan pada atribut ke i Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ (||x||_2) = (L^2- norm||x||_2= \\sqrt {\\sum_i=1^nx_i^2}) Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ keterangan : (s) = matrik covariance data Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan \u200b Mengukur Jarak Atribut Binary Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. erlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-data","text":"Pengukuran jarak merupakan basis dalam pemetaan.","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-numerik","text":"","title":"Mengukur Jarak Tipe Numerik"},{"location":"Mengukur Jarak Data/#minkowski-distance","text":"Minkowski Distanceyang merupakan generalisasi dari beberapa distance space yang ada seperti L1 (Manhattan/City Block) dan L2 (Euclidean), juga telah diimplementasikan. Minkowski menggunakan bilangan order \u03bb. $$ d _ { \\operatorname { min } } = ( \\sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ keterangan: (m) = bilangan riel positif (x_i) dan (y_i) = dua vektor dalam runang dimensi (n)","title":"Minkowski Distance"},{"location":"Mengukur Jarak Data/#manhattan-distance","text":"Manhattan Distance adalah merupakan salah satu teknik yang sering digunakan untuk menentukan kesamaan antara dua buah obyek. Pengukuran ini dihasilkan berdasarkan penjumlahan jarak selisih antara dua buah obyek dan hasil yang didapatkan dari Manhattan Distance bernilai mutlak. Dimana, Manhattan Distance melakukan perhitungan jarak dengan cara tegak lurus. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ keterangan : (x_i) = nilai X pada data latih. (y_i) = nilai Y pada data uji. m = batas jumlah banyaknya data Jika angka hasil dari rumus tersebut besar, maka tingkatkemiripan antara kedua objek akan semakin kecil dan sebaliknya, jika angka hasil rumus tersebut kecil, maka tingkat kemiripanantara kedua objek akan semakin besar. Objek yang dimaksudadalah data latih dan data uji yang akan dihitung tingkatkemiripannya.","title":"Manhattan distance"},{"location":"Mengukur Jarak Data/#euclidean-distance","text":"Euclidean distance adalah perhitungan jarak dari 2 buah titik dalam Euclidean space. Euclidean space diperkenalkan oleh Euclid, seorang matematikawan dari Yunani sekitar tahun 300 B.C.E. untuk mempelajari hubungan antara sudut dan jarak. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Berikut adalah persamaan EuclideanDistanceuntuk titik yang mempunyai ruang 3 dimensi (Negoro, 2015):","title":"Euclidean distance"},{"location":"Mengukur Jarak Data/#average-distance","text":"adalah modifikasi dari euclidean distance yang dibuat karena adanya kelemahan dari euclidean distance itu sendiri. $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ keterangan : (x,y) = titik koordinat dari vektor (n)= dimensi","title":"Average distance"},{"location":"Mengukur Jarak Data/#weighted-euclidean-distance","text":"adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ keterangan: (w_i) = bobot yang diberikan pada atribut ke i","title":"Weighted euclidean distance"},{"location":"Mengukur Jarak Data/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ (||x||_2) = (L^2- norm||x||_2= \\sqrt {\\sum_i=1^nx_i^2})","title":"Chord distance"},{"location":"Mengukur Jarak Data/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ keterangan : (s) = matrik covariance data","title":"Mahalanobis distance"},{"location":"Mengukur Jarak Data/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan \u200b Mengukur Jarak Atribut Binary Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t","title":"Cosine measure"},{"location":"Mengukur Jarak Data/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan","title":"Pearson correlation"},{"location":"Mengukur Jarak Data/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-categorical","text":"","title":"Mengukur Jarak Tipe categorical"},{"location":"Mengukur Jarak Data/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya.","title":"Overlay Metric"},{"location":"Mengukur Jarak Data/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"Mengukur Jarak Data/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. erlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Menghitung Jarak Tipe Campuran"},{"location":"Missing Value/","text":"Missing Value with K Nearest Neighbor \u00b6 Missing Data merupakan suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. Salah satu metode imputasi yaitu metode K Nearest Neighbor(KNN). Penelitian itu dilakukan untuk memprediksi seluruh nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K yaitu jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan seluruh nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15. Imputasi dengan Metode KNN \u00b6 Salah satu metode yang sering digunakan untuk permasalahan imputasi missing data adalah yaitu KNN. Metode ini merupakan metode sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan Nilai K Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan seluruh rata-rata k tetangga terdekat di Dc Kelemahan dan Keuntungan KNN \u00b6 Keuntungan KNN metode ini dapat bisa digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu sedikit membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 True True False 1 True True True 2 False True True 3 True False True","title":"Missing Value"},{"location":"Missing Value/#missing-value-with-k-nearest-neighbor","text":"Missing Data merupakan suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. Salah satu metode imputasi yaitu metode K Nearest Neighbor(KNN). Penelitian itu dilakukan untuk memprediksi seluruh nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K yaitu jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan seluruh nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15.","title":"Missing Value with K Nearest Neighbor"},{"location":"Missing Value/#imputasi-dengan-metode-knn","text":"Salah satu metode yang sering digunakan untuk permasalahan imputasi missing data adalah yaitu KNN. Metode ini merupakan metode sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan Nilai K Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan seluruh rata-rata k tetangga terdekat di Dc","title":"Imputasi dengan Metode KNN"},{"location":"Missing Value/#kelemahan-dan-keuntungan-knn","text":"Keuntungan KNN metode ini dapat bisa digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu sedikit membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 True True False 1 True True True 2 False True True 3 True False True","title":"Kelemahan dan Keuntungan KNN"},{"location":"Tugas 5 - Clustering (1)/","text":"CLUSTERING \u00b6 Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM). Clustering dengan pendekatan partisi \u00b6 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means \u00b6 Metode K-Modes \u00b6 \u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Metode K-Prototype \u00b6 \u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u00b6 \u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K-Prototype \u00b6 2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Clustering dengan Pendekatan Hirarki \u00b6 \u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa. Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM) \u00b6 \u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"Tugas 5 - Clustering (1)/#clustering","text":"Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM).","title":"CLUSTERING"},{"location":"Tugas 5 - Clustering (1)/#clustering-dengan-pendekatan-partisi","text":"1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Clustering dengan pendekatan partisi"},{"location":"Tugas 5 - Clustering (1)/#algoritma-k-means","text":"\u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"Tugas 5 - Clustering (1)/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"Tugas 5 - Clustering (1)/#metode-k-modes","text":"\u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Tugas 5 - Clustering (1)/#metode-k-prototype","text":"\u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Tugas 5 - Clustering (1)/#algoritma-k-prototype","text":"\u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Tugas 5 - Clustering (1)/#rumus-k-prototype","text":"2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K-Prototype"},{"location":"Tugas 5 - Clustering (1)/#clustering-dengan-pendekatan-hirarki","text":"\u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa.","title":"Clustering dengan Pendekatan Hirarki"},{"location":"Tugas 5 - Clustering (1)/#clustering-dengan-pendekatan-automatic-mapping-self-organising-mapsom","text":"\u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM)"},{"location":"Tugas 6 - Fuzzy C-Means/","text":"Fuzzy C-Means \u00b6 \u200b Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. \u200b Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. \u200b Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT). Algoritma fuzzy c-means \u00b6 from fcmeans import FCM from sklearn.datasets import make_blobs from matplotlib import pyplot as plt from seaborn import scatterplot as scatter # create artifitial dataset n_samples = 50000 n_bins = 3 # use 3 bins for calibration_curve as we have 3 clusters here centers = [( - 5 , - 5 ), ( 0 , 0 ), ( 5 , 5 )] X , _ = make_blobs ( n_samples = n_samples , n_features = 2 , cluster_std = 1.0 , centers = centers , shuffle = False , random_state = 42 ) # fit the fuzzy-c-means fcm = FCM ( n_clusters = 3 ) fcm . fit ( X ) # outputs fcm_centers = fcm . centers fcm_labels = fcm . u . argmax ( axis = 1 ) # plot result % matplotlib inline f , axes = plt . subplots ( 1 , 2 , figsize = ( 11 , 5 )) scatter ( X [:, 0 ], X [:, 1 ], ax = axes [ 0 ]) scatter ( X [:, 0 ], X [:, 1 ], ax = axes [ 1 ], hue = fcm_labels ) scatter ( fcm_centers [:, 0 ], fcm_centers [:, 1 ], ax = axes [ 1 ], marker = \"s\" , s = 200 ) plt . show () MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Fuzzy C-Means"},{"location":"Tugas 6 - Fuzzy C-Means/#fuzzy-c-means","text":"\u200b Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. \u200b Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. \u200b Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT).","title":"Fuzzy C-Means"},{"location":"Tugas 6 - Fuzzy C-Means/#algoritma-fuzzy-c-means","text":"from fcmeans import FCM from sklearn.datasets import make_blobs from matplotlib import pyplot as plt from seaborn import scatterplot as scatter # create artifitial dataset n_samples = 50000 n_bins = 3 # use 3 bins for calibration_curve as we have 3 clusters here centers = [( - 5 , - 5 ), ( 0 , 0 ), ( 5 , 5 )] X , _ = make_blobs ( n_samples = n_samples , n_features = 2 , cluster_std = 1.0 , centers = centers , shuffle = False , random_state = 42 ) # fit the fuzzy-c-means fcm = FCM ( n_clusters = 3 ) fcm . fit ( X ) # outputs fcm_centers = fcm . centers fcm_labels = fcm . u . argmax ( axis = 1 ) # plot result % matplotlib inline f , axes = plt . subplots ( 1 , 2 , figsize = ( 11 , 5 )) scatter ( X [:, 0 ], X [:, 1 ], ax = axes [ 0 ]) scatter ( X [:, 0 ], X [:, 1 ], ax = axes [ 1 ], hue = fcm_labels ) scatter ( fcm_centers [:, 0 ], fcm_centers [:, 1 ], ax = axes [ 1 ], marker = \"s\" , s = 200 ) plt . show () MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Algoritma fuzzy c-means"}]}