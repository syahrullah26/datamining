{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome To Data Mining \u00b6 Biodata \u00b6 Nama : Muhammad Syahrullah NIM : 180411100006 Kelas : Penambangan Data (5D) Jurusan : Teknik Informatika Angkatan : 2018 Alamat : Bekasi \u200b","title":"index"},{"location":"#welcome-to-data-mining","text":"","title":"Welcome To Data Mining"},{"location":"#biodata","text":"Nama : Muhammad Syahrullah NIM : 180411100006 Kelas : Penambangan Data (5D) Jurusan : Teknik Informatika Angkatan : 2018 Alamat : Bekasi \u200b","title":"Biodata"},{"location":"Decision Tree/","text":"DECISION TREE \u00b6 decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Decision Tree"},{"location":"Decision Tree/#decision-tree","text":"decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"Decision Tree/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"CARA MEMBUAT DECISION TREE"},{"location":"Isi/","text":"Statistika Deskriptif \u00b6 pengertian: Statistik Deskriptif adalah statistik yang berfungsi untuk mendeskripsikan atau memberi gambaran terhadap obyek yang diteliti melalui data sampel atau populasi sebagaimana adanya, tanpa melakukan analisis dan membuat kesimpulan yang berlaku untuk umum. Tipe Statistik Deskriptif : Mean \u00b6 \u200b Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Median \u00b6 \u200b Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Nilai tengah dari kelompok data n = banyak data Modus \u00b6 \u200b Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif Varians \u00b6 \u200b Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data Standard Deviasi \u00b6 \u200b Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Rumus Standar Deviasi: $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Dimana : xi = nilai x ke i x = rata rata n = ukuran sampel Skawness \u00b6 Skawness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan mirin atau tidaknya bentuk kurva suatu distribusi frekuensi. $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile \u00b6 Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3 $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = nilai quartil n = banyak data Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan : \u00b6 Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Langkah - Langkah \u00b6 PERTAMA \u00b6 Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd KEDUA \u00b6 Selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'data.csv' ) df setelah di run maka program akan menampilkan seperti berikut : stats MTK B.indonesia B.inggris Fisika Min 20 50 50 40 Max 100 100 100 100 Mean 59.798 74.834 74.398 69.672 Standard Deviasi 23.37 14.77 14.72 17.35 Variasi 546.34 218.02 216.64 300.97 SKewnes 0.01 0.02 0.06 -0.02 Quartile 1 40 62 61 55 Quartile 2 59 74 73 70.5 Quartile 3 80.25 88 88 84 Median 59 74 73 70.5 Modus 39 60 95 56 KETIGA \u00b6 Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) KEEMPAT \u00b6 Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistika"},{"location":"Isi/#statistika-deskriptif","text":"pengertian: Statistik Deskriptif adalah statistik yang berfungsi untuk mendeskripsikan atau memberi gambaran terhadap obyek yang diteliti melalui data sampel atau populasi sebagaimana adanya, tanpa melakukan analisis dan membuat kesimpulan yang berlaku untuk umum. Tipe Statistik Deskriptif :","title":"Statistika Deskriptif"},{"location":"Isi/#mean","text":"\u200b Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Mean"},{"location":"Isi/#median","text":"\u200b Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Nilai tengah dari kelompok data n = banyak data","title":"Median"},{"location":"Isi/#modus","text":"\u200b Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif","title":"Modus"},{"location":"Isi/#varians","text":"\u200b Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data","title":"Varians"},{"location":"Isi/#standard-deviasi","text":"\u200b Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Rumus Standar Deviasi: $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Dimana : xi = nilai x ke i x = rata rata n = ukuran sampel","title":"Standard Deviasi"},{"location":"Isi/#skawness","text":"Skawness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan mirin atau tidaknya bentuk kurva suatu distribusi frekuensi. $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skawness"},{"location":"Isi/#quartile","text":"Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3 $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = nilai quartil n = banyak data","title":"Quartile"},{"location":"Isi/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"Isi/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan :"},{"location":"Isi/#langkah-langkah","text":"","title":"Langkah - Langkah"},{"location":"Isi/#pertama","text":"Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd","title":"PERTAMA"},{"location":"Isi/#kedua","text":"Selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'data.csv' ) df setelah di run maka program akan menampilkan seperti berikut : stats MTK B.indonesia B.inggris Fisika Min 20 50 50 40 Max 100 100 100 100 Mean 59.798 74.834 74.398 69.672 Standard Deviasi 23.37 14.77 14.72 17.35 Variasi 546.34 218.02 216.64 300.97 SKewnes 0.01 0.02 0.06 -0.02 Quartile 1 40 62 61 55 Quartile 2 59 74 73 70.5 Quartile 3 80.25 88 88 84 Median 59 74 73 70.5 Modus 39 60 95 56","title":"KEDUA"},{"location":"Isi/#ketiga","text":"Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' )))","title":"KETIGA"},{"location":"Isi/#keempat","text":"Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"KEEMPAT"},{"location":"Mengukur Jarak Data/","text":"Mengukur Jarak Data \u00b6 Pengukuran jarak merupakan basis dalam pemetaan. Mengukur Jarak Tipe Numerik \u00b6 Minkowski Distance \u00b6 Minkowski Distanceyang merupakan generalisasi dari beberapa distance space yang ada seperti L1 (Manhattan/City Block) dan L2 (Euclidean), juga telah diimplementasikan. Minkowski menggunakan bilangan order \u03bb. $$ d _ { \\operatorname { min } } = ( \\sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ keterangan: (m) = bilangan riel positif (x_i) dan (y_i) = dua vektor dalam runang dimensi (n) Manhattan distance \u00b6 Manhattan Distance adalah merupakan salah satu teknik yang sering digunakan untuk menentukan kesamaan antara dua buah obyek. Pengukuran ini dihasilkan berdasarkan penjumlahan jarak selisih antara dua buah obyek dan hasil yang didapatkan dari Manhattan Distance bernilai mutlak. Dimana, Manhattan Distance melakukan perhitungan jarak dengan cara tegak lurus. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ keterangan : (x_i) = nilai X pada data latih. (y_i) = nilai Y pada data uji. m = batas jumlah banyaknya data Jika angka hasil dari rumus tersebut besar, maka tingkatkemiripan antara kedua objek akan semakin kecil dan sebaliknya, jika angka hasil rumus tersebut kecil, maka tingkat kemiripanantara kedua objek akan semakin besar. Objek yang dimaksudadalah data latih dan data uji yang akan dihitung tingkatkemiripannya. Euclidean distance \u00b6 Euclidean distance adalah perhitungan jarak dari 2 buah titik dalam Euclidean space. Euclidean space diperkenalkan oleh Euclid, seorang matematikawan dari Yunani sekitar tahun 300 B.C.E. untuk mempelajari hubungan antara sudut dan jarak. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Berikut adalah persamaan EuclideanDistanceuntuk titik yang mempunyai ruang 3 dimensi (Negoro, 2015): Average distance \u00b6 adalah modifikasi dari euclidean distance yang dibuat karena adanya kelemahan dari euclidean distance itu sendiri. $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ keterangan : (x,y) = titik koordinat dari vektor (n)= dimensi Weighted euclidean distance \u00b6 adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ keterangan: (w_i) = bobot yang diberikan pada atribut ke i Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ (||x||_2) = (L^2- norm||x||_2= \\sqrt {\\sum_i=1^nx_i^2}) Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ keterangan : (s) = matrik covariance data Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan \u200b Mengukur Jarak Atribut Binary Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. erlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-data","text":"Pengukuran jarak merupakan basis dalam pemetaan.","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-numerik","text":"","title":"Mengukur Jarak Tipe Numerik"},{"location":"Mengukur Jarak Data/#minkowski-distance","text":"Minkowski Distanceyang merupakan generalisasi dari beberapa distance space yang ada seperti L1 (Manhattan/City Block) dan L2 (Euclidean), juga telah diimplementasikan. Minkowski menggunakan bilangan order \u03bb. $$ d _ { \\operatorname { min } } = ( \\sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ keterangan: (m) = bilangan riel positif (x_i) dan (y_i) = dua vektor dalam runang dimensi (n)","title":"Minkowski Distance"},{"location":"Mengukur Jarak Data/#manhattan-distance","text":"Manhattan Distance adalah merupakan salah satu teknik yang sering digunakan untuk menentukan kesamaan antara dua buah obyek. Pengukuran ini dihasilkan berdasarkan penjumlahan jarak selisih antara dua buah obyek dan hasil yang didapatkan dari Manhattan Distance bernilai mutlak. Dimana, Manhattan Distance melakukan perhitungan jarak dengan cara tegak lurus. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ keterangan : (x_i) = nilai X pada data latih. (y_i) = nilai Y pada data uji. m = batas jumlah banyaknya data Jika angka hasil dari rumus tersebut besar, maka tingkatkemiripan antara kedua objek akan semakin kecil dan sebaliknya, jika angka hasil rumus tersebut kecil, maka tingkat kemiripanantara kedua objek akan semakin besar. Objek yang dimaksudadalah data latih dan data uji yang akan dihitung tingkatkemiripannya.","title":"Manhattan distance"},{"location":"Mengukur Jarak Data/#euclidean-distance","text":"Euclidean distance adalah perhitungan jarak dari 2 buah titik dalam Euclidean space. Euclidean space diperkenalkan oleh Euclid, seorang matematikawan dari Yunani sekitar tahun 300 B.C.E. untuk mempelajari hubungan antara sudut dan jarak. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Berikut adalah persamaan EuclideanDistanceuntuk titik yang mempunyai ruang 3 dimensi (Negoro, 2015):","title":"Euclidean distance"},{"location":"Mengukur Jarak Data/#average-distance","text":"adalah modifikasi dari euclidean distance yang dibuat karena adanya kelemahan dari euclidean distance itu sendiri. $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ keterangan : (x,y) = titik koordinat dari vektor (n)= dimensi","title":"Average distance"},{"location":"Mengukur Jarak Data/#weighted-euclidean-distance","text":"adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ keterangan: (w_i) = bobot yang diberikan pada atribut ke i","title":"Weighted euclidean distance"},{"location":"Mengukur Jarak Data/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ (||x||_2) = (L^2- norm||x||_2= \\sqrt {\\sum_i=1^nx_i^2})","title":"Chord distance"},{"location":"Mengukur Jarak Data/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ keterangan : (s) = matrik covariance data","title":"Mahalanobis distance"},{"location":"Mengukur Jarak Data/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan \u200b Mengukur Jarak Atribut Binary Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t","title":"Cosine measure"},{"location":"Mengukur Jarak Data/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan","title":"Pearson correlation"},{"location":"Mengukur Jarak Data/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-categorical","text":"","title":"Mengukur Jarak Tipe categorical"},{"location":"Mengukur Jarak Data/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya.","title":"Overlay Metric"},{"location":"Mengukur Jarak Data/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"Mengukur Jarak Data/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. erlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Menghitung Jarak Tipe Campuran"},{"location":"Missing Value/","text":"Missing Value with K Nearest Neighbor \u00b6 Missing Data merupakan suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. Salah satu metode imputasi yaitu metode K Nearest Neighbor(KNN). Penelitian itu dilakukan untuk memprediksi seluruh nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K yaitu jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan seluruh nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15. Imputasi dengan Metode KNN \u00b6 Salah satu metode yang sering digunakan untuk permasalahan imputasi missing data adalah yaitu KNN. Metode ini merupakan metode sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan Nilai K Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan seluruh rata-rata k tetangga terdekat di Dc Kelemahan dan Keuntungan KNN \u00b6 Keuntungan KNN metode ini dapat bisa digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu sedikit membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 True True False 1 True True True 2 False True True 3 True False True","title":"Missing Value"},{"location":"Missing Value/#missing-value-with-k-nearest-neighbor","text":"Missing Data merupakan suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. Salah satu metode imputasi yaitu metode K Nearest Neighbor(KNN). Penelitian itu dilakukan untuk memprediksi seluruh nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K yaitu jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan seluruh nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15.","title":"Missing Value with K Nearest Neighbor"},{"location":"Missing Value/#imputasi-dengan-metode-knn","text":"Salah satu metode yang sering digunakan untuk permasalahan imputasi missing data adalah yaitu KNN. Metode ini merupakan metode sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan Nilai K Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan seluruh rata-rata k tetangga terdekat di Dc","title":"Imputasi dengan Metode KNN"},{"location":"Missing Value/#kelemahan-dan-keuntungan-knn","text":"Keuntungan KNN metode ini dapat bisa digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu sedikit membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 True True False 1 True True True 2 False True True 3 True False True","title":"Kelemahan dan Keuntungan KNN"}]}